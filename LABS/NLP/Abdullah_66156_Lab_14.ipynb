{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Tokenization"
      ],
      "metadata": {
        "id": "iWNRjR5ZQeXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.chunk import RegexpParser\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G5yjikfRFWO",
        "outputId": "e4a4a006-7e49-4a14-82fd-f01cca8c03c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform word tokenization using NLTK"
      ],
      "metadata": {
        "id": "36kxC2zXQjWC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyQWjoSKQT_B",
        "outputId": "38ce4e30-3bac-4ca3-b061-503be562ac19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Riphah', 'University', 'is', 'located', 'in', 'Islamabad', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "\n",
        "text=\"Riphah University is located in Islamabad.\"\n",
        "\n",
        "words=word_tokenize(text)\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform sentence tokenization using NLTK"
      ],
      "metadata": {
        "id": "h1fqXJ5DRUGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=sent_tokenize(text)\n",
        "sentences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_ULEJMORXTe",
        "outputId": "d050e2bb-bcab-48fa-e669-5d1755d30106"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Riphah University is located in Islamabad.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Stopword Removal\n"
      ],
      "metadata": {
        "id": "oh2bpZHzRakV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to eliminate stopwords using NLTK."
      ],
      "metadata": {
        "id": "Xvy3vF5rRxtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=set(stopwords.words('english'))\n",
        "\n",
        "filtered_words=[w for w in words if w.lower() not in stop_words]\n",
        "filtered_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbRE9APkRopo",
        "outputId": "12a6731b-10f6-4209-bba5-6640b77f899a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Riphah', 'University', 'located', 'Islamabad', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Stemming"
      ],
      "metadata": {
        "id": "SsS5gxAMR6as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform stemming (Porter or Snowball) using NLTK."
      ],
      "metadata": {
        "id": "gR-wE5d6R77S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps=PorterStemmer()\n",
        "snow=SnowballStemmer(\"english\")\n",
        "\n",
        "stemmed_porter=[ps.stem(w) for w in words]\n",
        "stemmed_snowball=[snow.stem(w) for w in words]\n",
        "\n",
        "stemmed_porter,stemmed_snowball\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyFTHqC8R3aw",
        "outputId": "81025630-07f0-49e7-fd4a-bee9b67f2ffd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['riphah', 'univers', 'is', 'locat', 'in', 'islamabad', '.'],\n",
              " ['riphah', 'univers', 'is', 'locat', 'in', 'islamabad', '.'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Parts of Speech Tagging"
      ],
      "metadata": {
        "id": "OwsLW4-9SKFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform POS tagging using NLTK"
      ],
      "metadata": {
        "id": "c7CNtwL3SSf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "pos_result=pos_tag(words)\n",
        "pos_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHB7b8wXSOvn",
        "outputId": "3cd57c3c-0e3d-4cd1-b364-7f7276dfe3a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Riphah', 'NNP'),\n",
              " ('University', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('located', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('Islamabad', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Lemmatization\n"
      ],
      "metadata": {
        "id": "1KUA5Hs2Seme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program to perform lemmatization using WordNetLemmatizer."
      ],
      "metadata": {
        "id": "WAvyDaB-SjZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words=[lemmatizer.lemmatize(w) for w in words]\n",
        "lemmatized_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZJNZqypSmiY",
        "outputId": "6fa9a2e3-86e9-42b0-c407-e06fbb6d9005"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Riphah', 'University', 'is', 'located', 'in', 'Islamabad', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Chunking\n"
      ],
      "metadata": {
        "id": "X5ikekjKSrlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program for chunking using NLTK‚Äôs RegEx chunk grammar"
      ],
      "metadata": {
        "id": "EOy_OBuDSu-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling\n",
        "sentence=\"The quick brown fox jumps over the lazy dog\"\n",
        "tokens=word_tokenize(sentence)\n",
        "tagged=pos_tag(tokens)\n",
        "\n",
        "grammar=\"NP: {<DT>?<JJ>*<NN>}\"\n",
        "parser=RegexpParser(grammar)\n",
        "\n",
        "chunked_output=parser.parse(tagged)\n",
        "chunked_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "t_BXAddrSy6O",
        "outputId": "a7e2f9b5-c971-4ff9-8ab9-8ba1ad633464"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/67.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN')]), Tree('NP', [('fox', 'NN')]), ('jumps', 'VBZ'), ('over', 'IN'), Tree('NP', [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,424.0,168.0\" width=\"424px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"35.8491%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"26.3158%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.1579%\" y1=\"20px\" y2=\"48px\" /><svg width=\"36.8421%\" x=\"26.3158%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">quick</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"44.7368%\" y1=\"20px\" y2=\"48px\" /><svg width=\"36.8421%\" x=\"63.1579%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">brown</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.5789%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.9245%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.43396%\" x=\"35.8491%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">fox</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.566%\" y1=\"20px\" y2=\"48px\" /><svg width=\"13.2075%\" x=\"45.283%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">jumps</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.8868%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.3208%\" x=\"58.4906%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">over</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.1509%\" y1=\"20px\" y2=\"48px\" /><svg width=\"30.1887%\" x=\"69.8113%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"31.25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.625%\" y1=\"20px\" y2=\"48px\" /><svg width=\"37.5%\" x=\"31.25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">lazy</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /><svg width=\"31.25%\" x=\"68.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.375%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.9057%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Named Entity Recognition\n"
      ],
      "metadata": {
        "id": "iv-ErYSqS_BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚Ä¢ Write a Python program for NER using NLTK‚Äôs ne_chunk()."
      ],
      "metadata": {
        "id": "S1M-6joQTBu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2=\"Albert Einstein was born in Germany.\"\n",
        "\n",
        "tokens=word_tokenize(text2)\n",
        "tagged=pos_tag(tokens)\n",
        "\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "ner_output=ne_chunk(tagged)\n",
        "ner_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "doWm6-sDTFFh",
        "outputId": "c5dbc7ae-eec5-486e-81c0-d8dfa8ef55c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('PERSON', [('Albert', 'NNP')]), Tree('PERSON', [('Einstein', 'NNP')]), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), Tree('GPE', [('Germany', 'NNP')]), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,360.0,168.0\" width=\"360px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"17.7778%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Albert</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.88889%\" y1=\"20px\" y2=\"48px\" /><svg width=\"22.2222%\" x=\"17.7778%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Einstein</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.8889%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.1111%\" x=\"40%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.5556%\" y1=\"20px\" y2=\"48px\" /><svg width=\"13.3333%\" x=\"51.1111%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">born</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.7778%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.88889%\" x=\"64.4444%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.8889%\" y1=\"20px\" y2=\"48px\" /><svg width=\"20%\" x=\"73.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Germany</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.66667%\" x=\"93.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.6667%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. TF‚ÄìIDF Calculation\n",
        "##‚Ä¢ Write a Python program to compute:\n",
        "##o Term Frequency (TF)\n",
        "##o Inverse Document Frequency (IDF)\n",
        "##o TF‚ÄìIDF values"
      ],
      "metadata": {
        "id": "oeJl5CfqTOzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs=[\n",
        "    \"AI is the future of technology.\",\n",
        "    \"Machine learning is part of AI.\",\n",
        "    \"Deep learning advances AI research.\"\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "D8D9jCE_TiXf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîπ Step 1 ‚Äî Compute Term Frequency (TF)"
      ],
      "metadata": {
        "id": "X3gfdEa4T511"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv=CountVectorizer()\n",
        "tf_matrix=cv.fit_transform(docs)\n",
        "\n",
        "print(\"TF Matrix:\")\n",
        "print(tf_matrix.toarray())\n",
        "print(\"\\nVocabulary:\")\n",
        "print(cv.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNLmyacRT3ly",
        "outputId": "d97cd332-f993-423f-f41c-a52c04071a75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Matrix:\n",
            "[[0 1 0 1 1 0 0 1 0 0 1 1]\n",
            " [0 1 0 0 1 1 1 1 1 0 0 0]\n",
            " [1 1 1 0 0 1 0 0 0 1 0 0]]\n",
            "\n",
            "Vocabulary:\n",
            "['advances' 'ai' 'deep' 'future' 'is' 'learning' 'machine' 'of' 'part'\n",
            " 'research' 'technology' 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîπ Step 2 ‚Äî Compute Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "X0uFb6VFT9_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer()\n",
        "tfidf_transformer.fit(tf_matrix)\n",
        "\n",
        "print(\"\\nIDF Values:\")\n",
        "for word,idf_val in zip(cv.get_feature_names_out(),tfidf_transformer.idf_):\n",
        "    print(word,\":\",idf_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p914A9YYUCM6",
        "outputId": "0856d263-3fa1-42b7-f80b-2763a24cdbc4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IDF Values:\n",
            "advances : 1.6931471805599454\n",
            "ai : 1.0\n",
            "deep : 1.6931471805599454\n",
            "future : 1.6931471805599454\n",
            "is : 1.2876820724517808\n",
            "learning : 1.2876820724517808\n",
            "machine : 1.6931471805599454\n",
            "of : 1.2876820724517808\n",
            "part : 1.6931471805599454\n",
            "research : 1.6931471805599454\n",
            "technology : 1.6931471805599454\n",
            "the : 1.6931471805599454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîπ Step 3 ‚Äî Compute TF-IDF Values"
      ],
      "metadata": {
        "id": "9-vWWAypUD2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix=tfidf_transformer.transform(tf_matrix)\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bccaRGl3UG0k",
        "outputId": "6934350f-ed93-4b9c-e34b-d54933b3fcfc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.27824521 0.         0.4711101  0.35829137 0.\n",
            "  0.         0.35829137 0.         0.         0.4711101  0.4711101 ]\n",
            " [0.         0.2922544  0.         0.         0.37633075 0.37633075\n",
            "  0.49482971 0.37633075 0.49482971 0.         0.         0.        ]\n",
            " [0.50461134 0.29803159 0.50461134 0.         0.         0.38376993\n",
            "  0.         0.         0.         0.50461134 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus 1: Sentiment Analysis\n",
        "##‚Ä¢ Write Python code to perform sentiment analysis using NLP (VADER or TextBlob)"
      ],
      "metadata": {
        "id": "jp0t2QAtULTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia=SentimentIntensityAnalyzer()\n",
        "\n",
        "sentence=\"I absolutely love Artificial Intelligence!\"\n",
        "sia.polarity_scores(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzhRgoymUR7-",
        "outputId": "a58ba2eb-de7f-4a78-b8d4-0c8fd49a7a46"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.197, 'pos': 0.803, 'compound': 0.8461}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus 2: Spam Filter\n",
        "##‚Ä¢ Write Python code to develop a Spam Detection model using NLP."
      ],
      "metadata": {
        "id": "cQA_QPAqUVK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "data=[\n",
        "    (\"Win a free iPhone now!\", \"spam\"),\n",
        "    (\"Your appointment is confirmed.\", \"ham\"),\n",
        "    (\"Congratulations! You won a lottery!\", \"spam\"),\n",
        "    (\"Please submit the assignment today.\", \"ham\")\n",
        "]\n",
        "\n",
        "texts=[x[0] for x in data]\n",
        "labels=[x[1] for x in data]\n",
        "\n",
        "cv=CountVectorizer()\n",
        "X=cv.fit_transform(texts)\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,labels,test_size=0.25)\n",
        "\n",
        "model=MultinomialNB()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "pred=model.predict(X_test)\n",
        "accuracy_score(y_test, pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RczveUlzUacf",
        "outputId": "474ca8ea-743e-4439-8576-62cd8e8c92c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus 3: Fake News Detection\n",
        "##‚Ä¢ Write Python code to build a Fake News Classification model using NLP and ML."
      ],
      "metadata": {
        "id": "-j5AnVvGUc8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "docs=[\n",
        "    \"Breaking: Aliens landed in New York!\",\n",
        "    \"PM announces new economic reforms.\",\n",
        "    \"Scientists discover cure for aging!\",\n",
        "    \"Weather report predicts rain tomorrow.\"\n",
        "]\n",
        "\n",
        "labels=[\"fake\", \"real\", \"fake\", \"real\"]\n",
        "\n",
        "tfidf=TfidfVectorizer()\n",
        "X=tfidf.fit_transform(docs)\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,labels,test_size=0.25)\n",
        "\n",
        "classifier=LogisticRegression()\n",
        "classifier.fit(X_train,y_train)\n",
        "\n",
        "preds=classifier.predict(X_test)\n",
        "accuracy_score(y_test,preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-Ypyk2JUihb",
        "outputId": "9dabfb5f-a784-45a7-d49a-2964614e9cde"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}